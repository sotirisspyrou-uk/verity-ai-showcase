#!/usr/bin/env python3
"""
Regulatory Reporting Suite - Professional Portfolio Demo
Created by Sotirios Spyrou (https://www.linkedin.com/in/sspyrou/)

Demonstrates automated regulatory reporting for AI systems across multiple frameworks.
Essential for Chief Compliance Officers, Risk Managers, and Board Directors.

Target Audience: C-Suite Executives, Board Directors, Regulatory Affairs, Risk Officers
Strategic Value: Transforms complex regulatory requirements into executive-ready insights

DISCLAIMER: This is demonstration code for portfolio purposes. 
Production implementations require regulatory consultation and customization.
Contact: https://verityai.co for enterprise regulatory solutions.
"""

import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import uuid


class RegulatoryFramework(Enum):
    """Major AI regulatory frameworks requiring reporting"""
    EU_AI_ACT = "eu_ai_act"
    GDPR = "gdpr"
    ISO_42001 = "iso_42001"
    NIST_AI_RMF = "nist_ai_rmf"
    SOX = "sarbanes_oxley"
    BASEL_III = "basel_iii"
    FDA_AI_ML = "fda_ai_ml"
    CCPA = "ccpa"
    PCI_DSS = "pci_dss"


class ComplianceStatus(Enum):
    """Compliance assessment status levels"""
    FULLY_COMPLIANT = "fully_compliant"
    SUBSTANTIALLY_COMPLIANT = "substantially_compliant"
    PARTIALLY_COMPLIANT = "partially_compliant"
    NON_COMPLIANT = "non_compliant"
    NOT_APPLICABLE = "not_applicable"


class RiskLevel(Enum):
    """Regulatory risk classification"""
    MINIMAL = "minimal"
    LIMITED = "limited"
    HIGH = "high"
    UNACCEPTABLE = "unacceptable"


@dataclass
class ComplianceAssessment:
    """Individual compliance framework assessment results"""
    framework: RegulatoryFramework
    status: ComplianceStatus
    score: float  # 0-100 compliance score
    last_assessment: str
    next_due: str
    critical_gaps: List[str]
    recommendations: List[str]
    risk_level: RiskLevel
    estimated_fine_exposure: str


@dataclass
class RegulatoryReport:
    """Comprehensive regulatory report structure"""
    report_id: str
    generation_date: str
    reporting_period: Dict[str, str]
    ai_system_id: str
    system_name: str
    framework_assessments: List[ComplianceAssessment]
    overall_compliance_score: float
    executive_summary: str
    priority_actions: List[Dict[str, Any]]
    regulatory_timeline: List[Dict[str, Any]]


class RegulatoryReportingSuite:
    """
    Enterprise regulatory reporting automation for AI systems
    
    Key Capabilities:
    - Multi-framework compliance assessment
    - Executive-ready regulatory reports
    - Automated compliance scoring
    - Risk-based prioritization
    - Board presentation materials
    """
    
    def __init__(self):
        self.compliance_frameworks = self._load_compliance_frameworks()
        self.assessment_history = []
        self.regulatory_calendar = self._load_regulatory_calendar()
        
        print("üìä Regulatory Reporting Suite Initialized")
        print("üèõÔ∏è Ready for enterprise compliance reporting")
    
    def _load_compliance_frameworks(self) -> Dict[RegulatoryFramework, Dict[str, Any]]:
        """Load regulatory framework specifications and requirements"""
        return {
            RegulatoryFramework.EU_AI_ACT: {
                "name": "EU AI Act",
                "authority": "European Commission",
                "scope": "AI systems in EU market",
                "reporting_frequency": "Annual",
                "key_requirements": [
                    "Risk classification and assessment",
                    "Conformity assessment procedures",
                    "Quality management systems",
                    "Human oversight requirements",
                    "Transparency and explainability"
                ],
                "penalty_range": "Up to ‚Ç¨35M or 7% of global turnover",
                "assessment_criteria": {
                    "risk_assessment": 25,
                    "quality_management": 20,
                    "human_oversight": 20,
                    "transparency": 15,
                    "documentation": 20
                }
            },
            RegulatoryFramework.GDPR: {
                "name": "General Data Protection Regulation",
                "authority": "EU Data Protection Authorities",
                "scope": "Personal data processing",
                "reporting_frequency": "As required",
                "key_requirements": [
                    "Lawful basis for processing",
                    "Data subject rights implementation",
                    "Privacy by design principles",
                    "Data breach notification",
                    "Data protection impact assessments"
                ],
                "penalty_range": "Up to ‚Ç¨20M or 4% of global turnover",
                "assessment_criteria": {
                    "lawful_basis": 30,
                    "data_subject_rights": 25,
                    "privacy_by_design": 20,
                    "breach_procedures": 15,
                    "impact_assessments": 10
                }
            },
            RegulatoryFramework.ISO_42001: {
                "name": "ISO/IEC 42001 - AI Management Systems",
                "authority": "International Organization for Standardization",
                "scope": "AI management systems",
                "reporting_frequency": "Annual certification review",
                "key_requirements": [
                    "AI management system establishment",
                    "Risk management processes",
                    "Performance monitoring",
                    "Continuous improvement",
                    "Stakeholder engagement"
                ],
                "penalty_range": "Certification suspension/withdrawal",
                "assessment_criteria": {
                    "management_system": 25,
                    "risk_management": 25,
                    "performance_monitoring": 20,
                    "improvement_processes": 15,
                    "stakeholder_engagement": 15
                }
            },
            RegulatoryFramework.NIST_AI_RMF: {
                "name": "NIST AI Risk Management Framework",
                "authority": "National Institute of Standards and Technology",
                "scope": "AI risk management (US Federal)",
                "reporting_frequency": "Continuous",
                "key_requirements": [
                    "AI risk governance",
                    "Risk assessment and mitigation",
                    "AI system trustworthiness",
                    "Bias and fairness evaluation",
                    "Human-AI configuration"
                ],
                "penalty_range": "Federal contract restrictions",
                "assessment_criteria": {
                    "governance": 30,
                    "risk_assessment": 25,
                    "trustworthiness": 20,
                    "bias_evaluation": 15,
                    "human_ai_config": 10
                }
            }
        }
    
    def _load_regulatory_calendar(self) -> List[Dict[str, Any]]:
        """Load upcoming regulatory deadlines and milestones"""
        current_date = datetime.now()
        return [
            {
                "framework": RegulatoryFramework.EU_AI_ACT,
                "milestone": "AI Act Full Application",
                "date": "2026-08-02",
                "description": "All AI Act provisions fully applicable",
                "preparation_required": True
            },
            {
                "framework": RegulatoryFramework.ISO_42001,
                "milestone": "Annual Certification Review",
                "date": (current_date + timedelta(days=90)).strftime("%Y-%m-%d"),
                "description": "ISO 42001 certification renewal",
                "preparation_required": True
            },
            {
                "framework": RegulatoryFramework.GDPR,
                "milestone": "DPIA Review Cycle",
                "date": (current_date + timedelta(days=180)).strftime("%Y-%m-%d"),
                "description": "Data Protection Impact Assessment review",
                "preparation_required": True
            }
        ]
    
    def assess_system_compliance(self, ai_system_config: Dict[str, Any], 
                                frameworks: List[RegulatoryFramework] = None) -> List[ComplianceAssessment]:
        """Assess AI system compliance across specified regulatory frameworks"""
        
        if frameworks is None:
            frameworks = [RegulatoryFramework.EU_AI_ACT, RegulatoryFramework.GDPR, 
                         RegulatoryFramework.ISO_42001]
        
        assessments = []
        
        print(f"\nüîç Assessing compliance for: {ai_system_config.get('name', 'AI System')}")
        
        for framework in frameworks:
            assessment = self._assess_framework_compliance(framework, ai_system_config)
            assessments.append(assessment)
            print(f"   {framework.value.upper()}: {assessment.score:.1f}% - {assessment.status.value}")
        
        return assessments
    
    def _assess_framework_compliance(self, framework: RegulatoryFramework, 
                                   ai_config: Dict[str, Any]) -> ComplianceAssessment:
        """Assess compliance with specific regulatory framework"""
        
        framework_spec = self.compliance_frameworks[framework]
        criteria = framework_spec["assessment_criteria"]
        
        # Simulate compliance assessment based on AI system configuration
        # In production: integrate with actual compliance data and audits
        
        total_score = 0
        total_weight = sum(criteria.values())
        critical_gaps = []
        recommendations = []
        
        if framework == RegulatoryFramework.EU_AI_ACT:
            risk_assessment_score = 85 if ai_config.get("risk_assessment_completed") else 30
            quality_mgmt_score = 80 if ai_config.get("quality_management_system") else 25
            human_oversight_score = 90 if ai_config.get("human_oversight") else 20
            transparency_score = 75 if ai_config.get("explainable_ai") else 35
            documentation_score = 88 if ai_config.get("documentation_complete") else 40
            
            scores = {
                "risk_assessment": risk_assessment_score,
                "quality_management": quality_mgmt_score,
                "human_oversight": human_oversight_score,
                "transparency": transparency_score,
                "documentation": documentation_score
            }
            
            for criterion, score in scores.items():
                total_score += score * criteria[criterion] / 100
                if score < 60:
                    critical_gaps.append(f"EU AI Act: {criterion.replace('_', ' ').title()} below standard")
            
        elif framework == RegulatoryFramework.GDPR:
            lawful_basis_score = 90 if ai_config.get("legal_basis_documented") else 20
            rights_score = 85 if ai_config.get("data_subject_rights_implemented") else 30
            privacy_design_score = 80 if ai_config.get("privacy_by_design") else 35
            breach_score = 95 if ai_config.get("breach_procedures") else 50
            dpia_score = 75 if ai_config.get("dpia_completed") else 25
            
            scores = {
                "lawful_basis": lawful_basis_score,
                "data_subject_rights": rights_score,
                "privacy_by_design": privacy_design_score,
                "breach_procedures": breach_score,
                "impact_assessments": dpia_score
            }
            
            for criterion, score in scores.items():
                total_score += score * criteria[criterion] / 100
                if score < 60:
                    critical_gaps.append(f"GDPR: {criterion.replace('_', ' ').title()} requires attention")
        
        else:
            # Simplified scoring for other frameworks
            base_score = 75 if ai_config.get("compliant", False) else 45
            total_score = base_score
        
        # Determine compliance status
        if total_score >= 90:
            status = ComplianceStatus.FULLY_COMPLIANT
        elif total_score >= 75:
            status = ComplianceStatus.SUBSTANTIALLY_COMPLIANT
        elif total_score >= 50:
            status = ComplianceStatus.PARTIALLY_COMPLIANT
        else:
            status = ComplianceStatus.NON_COMPLIANT
        
        # Determine risk level
        if total_score >= 85:
            risk_level = RiskLevel.MINIMAL
        elif total_score >= 70:
            risk_level = RiskLevel.LIMITED
        elif total_score >= 50:
            risk_level = RiskLevel.HIGH
        else:
            risk_level = RiskLevel.UNACCEPTABLE
        
        # Generate recommendations
        if total_score < 75:
            recommendations.append(f"Prioritize {framework.value.replace('_', ' ').title()} compliance improvements")
        if critical_gaps:
            recommendations.append("Address critical compliance gaps within 30 days")
        if total_score < 50:
            recommendations.append("Consider system deployment suspension until compliance achieved")
        
        return ComplianceAssessment(
            framework=framework,
            status=status,
            score=total_score,
            last_assessment=datetime.now().strftime("%Y-%m-%d"),
            next_due=(datetime.now() + timedelta(days=90)).strftime("%Y-%m-%d"),
            critical_gaps=critical_gaps,
            recommendations=recommendations,
            risk_level=risk_level,
            estimated_fine_exposure=self._calculate_fine_exposure(framework, total_score)
        )
    
    def _calculate_fine_exposure(self, framework: RegulatoryFramework, compliance_score: float) -> str:
        """Calculate potential regulatory fine exposure based on compliance score"""
        
        framework_penalties = {
            RegulatoryFramework.EU_AI_ACT: {
                "high_risk": "‚Ç¨7-35M or 1.5-7% of global turnover",
                "medium_risk": "‚Ç¨15M or 3% of global turnover",
                "low_risk": "Administrative warnings"
            },
            RegulatoryFramework.GDPR: {
                "high_risk": "‚Ç¨20M or 4% of global turnover",
                "medium_risk": "‚Ç¨10M or 2% of global turnover", 
                "low_risk": "Corrective action orders"
            }
        }
        
        penalties = framework_penalties.get(framework, {
            "high_risk": "Significant regulatory penalties",
            "medium_risk": "Moderate penalties possible",
            "low_risk": "Minimal penalty risk"
        })
        
        if compliance_score < 50:
            return penalties["high_risk"]
        elif compliance_score < 75:
            return penalties["medium_risk"]
        else:
            return penalties["low_risk"]
    
    def generate_regulatory_report(self, ai_system_config: Dict[str, Any],
                                 frameworks: List[RegulatoryFramework] = None,
                                 reporting_period_days: int = 90) -> RegulatoryReport:
        """Generate comprehensive regulatory compliance report"""
        
        # Assess compliance across frameworks
        assessments = self.assess_system_compliance(ai_system_config, frameworks)
        
        # Calculate overall compliance score
        overall_score = sum(a.score for a in assessments) / len(assessments) if assessments else 0
        
        # Generate executive summary
        executive_summary = self._generate_executive_summary(assessments, overall_score)
        
        # Generate priority actions
        priority_actions = self._generate_priority_actions(assessments)
        
        # Create regulatory timeline
        regulatory_timeline = self._create_regulatory_timeline(assessments)
        
        report = RegulatoryReport(
            report_id=str(uuid.uuid4()),
            generation_date=datetime.now().isoformat(),
            reporting_period={
                "start": (datetime.now() - timedelta(days=reporting_period_days)).strftime("%Y-%m-%d"),
                "end": datetime.now().strftime("%Y-%m-%d")
            },
            ai_system_id=ai_system_config.get("system_id", "unknown"),
            system_name=ai_system_config.get("name", "AI System"),
            framework_assessments=assessments,
            overall_compliance_score=overall_score,
            executive_summary=executive_summary,
            priority_actions=priority_actions,
            regulatory_timeline=regulatory_timeline
        )
        
        # Store in assessment history
        self.assessment_history.append(report)
        
        return report
    
    def _generate_executive_summary(self, assessments: List[ComplianceAssessment], 
                                   overall_score: float) -> str:
        """Generate executive summary for regulatory report"""
        
        compliant_count = len([a for a in assessments if a.status in [ComplianceStatus.FULLY_COMPLIANT, 
                                                                       ComplianceStatus.SUBSTANTIALLY_COMPLIANT]])
        total_count = len(assessments)
        
        critical_frameworks = [a.framework.value for a in assessments if a.risk_level in [RiskLevel.HIGH, RiskLevel.UNACCEPTABLE]]
        
        summary = f"AI system regulatory compliance assessment across {total_count} frameworks shows "
        
        if overall_score >= 85:
            summary += f"excellent compliance ({overall_score:.1f}%) with {compliant_count}/{total_count} frameworks meeting standards."
        elif overall_score >= 70:
            summary += f"good compliance ({overall_score:.1f}%) with {compliant_count}/{total_count} frameworks meeting standards."
        elif overall_score >= 50:
            summary += f"moderate compliance ({overall_score:.1f}%) requiring focused improvement efforts."
        else:
            summary += f"significant compliance gaps ({overall_score:.1f}%) requiring immediate remediation."
        
        if critical_frameworks:
            summary += f" Critical attention required for: {', '.join(critical_frameworks).upper()}."
        
        return summary
    
    def _generate_priority_actions(self, assessments: List[ComplianceAssessment]) -> List[Dict[str, Any]]:
        """Generate priority action items from compliance assessments"""
        actions = []
        
        # Critical compliance gaps
        critical_assessments = [a for a in assessments if a.risk_level == RiskLevel.UNACCEPTABLE]
        for assessment in critical_assessments:
            actions.append({
                "priority": "CRITICAL",
                "framework": assessment.framework.value.upper(),
                "action": f"Address critical compliance gaps in {assessment.framework.value.replace('_', ' ').title()}",
                "timeline": "14 days",
                "owner": "Chief Compliance Officer",
                "estimated_effort": "High",
                "potential_impact": assessment.estimated_fine_exposure
            })
        
        # High-risk assessments
        high_risk_assessments = [a for a in assessments if a.risk_level == RiskLevel.HIGH]
        for assessment in high_risk_assessments:
            actions.append({
                "priority": "HIGH",
                "framework": assessment.framework.value.upper(),
                "action": f"Improve {assessment.framework.value.replace('_', ' ').title()} compliance score",
                "timeline": "30 days",
                "owner": "Compliance Team",
                "estimated_effort": "Medium",
                "potential_impact": assessment.estimated_fine_exposure
            })
        
        # Upcoming regulatory deadlines
        upcoming_deadlines = [d for d in self.regulatory_calendar 
                             if datetime.strptime(d["date"], "%Y-%m-%d") <= datetime.now() + timedelta(days=180)]
        for deadline in upcoming_deadlines:
            actions.append({
                "priority": "MEDIUM",
                "framework": deadline["framework"].value.upper(),
                "action": f"Prepare for {deadline['milestone']}",
                "timeline": deadline["date"],
                "owner": "Legal & Compliance",
                "estimated_effort": "Medium",
                "potential_impact": "Regulatory deadline compliance"
            })
        
        return sorted(actions, key=lambda x: {"CRITICAL": 1, "HIGH": 2, "MEDIUM": 3, "LOW": 4}[x["priority"]])
    
    def _create_regulatory_timeline(self, assessments: List[ComplianceAssessment]) -> List[Dict[str, Any]]:
        """Create regulatory compliance timeline"""
        timeline = []
        
        # Add assessment due dates
        for assessment in assessments:
            timeline.append({
                "date": assessment.next_due,
                "framework": assessment.framework.value.upper(),
                "event": "Compliance Assessment Due",
                "status": "upcoming",
                "importance": "high" if assessment.risk_level in [RiskLevel.HIGH, RiskLevel.UNACCEPTABLE] else "medium"
            })
        
        # Add regulatory milestones
        for milestone in self.regulatory_calendar:
            timeline.append({
                "date": milestone["date"],
                "framework": milestone["framework"].value.upper(),
                "event": milestone["milestone"],
                "status": "upcoming",
                "importance": "critical" if milestone["preparation_required"] else "medium"
            })
        
        # Sort by date
        return sorted(timeline, key=lambda x: x["date"])
    
    def generate_board_presentation(self, regulatory_report: RegulatoryReport) -> Dict[str, Any]:
        """Generate board-level presentation materials from regulatory report"""
        
        return {
            "slide_1_executive_summary": {
                "title": "AI Regulatory Compliance Overview",
                "overall_score": f"{regulatory_report.overall_compliance_score:.0f}%",
                "status_indicator": "üü¢" if regulatory_report.overall_compliance_score >= 85 else "üü°" if regulatory_report.overall_compliance_score >= 70 else "üî¥",
                "key_message": regulatory_report.executive_summary,
                "frameworks_assessed": len(regulatory_report.framework_assessments)
            },
            "slide_2_risk_assessment": {
                "title": "Regulatory Risk Profile",
                "high_risk_frameworks": [a.framework.value.upper() for a in regulatory_report.framework_assessments if a.risk_level in [RiskLevel.HIGH, RiskLevel.UNACCEPTABLE]],
                "maximum_fine_exposure": max([a.estimated_fine_exposure for a in regulatory_report.framework_assessments], key=len),
                "critical_actions_required": len([a for a in regulatory_report.priority_actions if a["priority"] == "CRITICAL"])
            },
            "slide_3_compliance_scorecard": {
                "title": "Framework Compliance Scores",
                "framework_scores": {a.framework.value.upper(): f"{a.score:.0f}%" for a in regulatory_report.framework_assessments},
                "trend_indicator": "‚ÜóÔ∏è",  # Simulated - in production, calculate from historical data
                "next_assessment_due": min([a.next_due for a in regulatory_report.framework_assessments])
            },
            "slide_4_action_plan": {
                "title": "Priority Actions & Timeline",
                "critical_actions": [a for a in regulatory_report.priority_actions if a["priority"] == "CRITICAL"][:3],
                "next_30_days": [a for a in regulatory_report.priority_actions if "30 days" in a.get("timeline", "")][:3],
                "resource_requirements": "Compliance team + legal counsel + technical resources"
            },
            "slide_5_regulatory_calendar": {
                "title": "Upcoming Regulatory Milestones",
                "next_90_days": [t for t in regulatory_report.regulatory_timeline if datetime.strptime(t["date"], "%Y-%m-%d") <= datetime.now() + timedelta(days=90)][:5],
                "critical_deadlines": len([t for t in regulatory_report.regulatory_timeline if t["importance"] == "critical"])
            }
        }
    
    def export_compliance_data(self, format_type: str = "json") -> str:
        """Export compliance assessment data for external systems"""
        
        export_data = {
            "export_metadata": {
                "generation_timestamp": datetime.now().isoformat(),
                "total_assessments": len(self.assessment_history),
                "frameworks_covered": list(set(f.value for report in self.assessment_history for f in [a.framework for a in report.framework_assessments])),
                "export_format": format_type
            },
            "compliance_history": []
        }
        
        # Convert reports to exportable format
        for report in self.assessment_history:
            report_dict = asdict(report)
            # Convert enums to strings for JSON serialization
            for assessment in report_dict["framework_assessments"]:
                assessment["framework"] = assessment["framework"].value
                assessment["status"] = assessment["status"].value
                assessment["risk_level"] = assessment["risk_level"].value
            
            export_data["compliance_history"].append(report_dict)
        
        return json.dumps(export_data, indent=2, sort_keys=True)


def demonstrate_regulatory_reporting():
    """Portfolio demonstration of regulatory reporting capabilities"""
    print("\n" + "="*70)
    print("üèõÔ∏è REGULATORY REPORTING SUITE - PORTFOLIO DEMONSTRATION")
    print("üìä Showcasing Enterprise Regulatory Compliance Capabilities")
    print("üë®‚Äçüíº Created by Sotirios Spyrou - Technical Marketing Leader")
    print("="*70)
    
    # Initialize reporting suite
    reporting_suite = RegulatoryReportingSuite()
    
    # Define sample AI system for assessment
    sample_ai_system = {
        "system_id": "financial_risk_ai",
        "name": "Financial Risk Assessment AI",
        "description": "AI system for automated credit risk and fraud detection",
        "risk_assessment_completed": True,
        "quality_management_system": True,
        "human_oversight": True,
        "explainable_ai": False,  # Gap for demonstration
        "documentation_complete": True,
        "legal_basis_documented": True,
        "data_subject_rights_implemented": True,
        "privacy_by_design": True,
        "breach_procedures": True,
        "dpia_completed": True,
        "compliant": True
    }
    
    print(f"\nüîç 1. ASSESSING AI SYSTEM: {sample_ai_system['name']}")
    print(f"üìã System Type: {sample_ai_system['description']}")
    
    # Generate comprehensive regulatory report
    regulatory_report = reporting_suite.generate_regulatory_report(
        ai_system_config=sample_ai_system,
        frameworks=[RegulatoryFramework.EU_AI_ACT, RegulatoryFramework.GDPR, RegulatoryFramework.ISO_42001]
    )
    
    print(f"\nüìä 2. REGULATORY COMPLIANCE RESULTS")
    print(f"   Overall Compliance Score: {regulatory_report.overall_compliance_score:.1f}%")
    print(f"   Frameworks Assessed: {len(regulatory_report.framework_assessments)}")
    print(f"   Report ID: {regulatory_report.report_id[:8]}...")
    
    # Display framework-specific results
    print(f"\nüéØ 3. FRAMEWORK-SPECIFIC ASSESSMENT")
    for assessment in regulatory_report.framework_assessments:
        status_icon = "‚úÖ" if assessment.status in [ComplianceStatus.FULLY_COMPLIANT, ComplianceStatus.SUBSTANTIALLY_COMPLIANT] else "‚ö†Ô∏è" if assessment.status == ComplianceStatus.PARTIALLY_COMPLIANT else "‚ùå"
        print(f"   {status_icon} {assessment.framework.value.upper()}: {assessment.score:.1f}% - {assessment.status.value}")
        print(f"      Risk Level: {assessment.risk_level.value.title()}")
        print(f"      Fine Exposure: {assessment.estimated_fine_exposure}")
    
    # Show executive summary
    print(f"\nüí° 4. EXECUTIVE SUMMARY")
    print(f"   {regulatory_report.executive_summary}")
    
    # Display priority actions
    print(f"\n‚ö° 5. PRIORITY ACTIONS ({len(regulatory_report.priority_actions)})")
    for action in regulatory_report.priority_actions[:3]:  # Show top 3
        priority_icon = "üî¥" if action["priority"] == "CRITICAL" else "üü†" if action["priority"] == "HIGH" else "üü°"
        print(f"   {priority_icon} [{action['priority']}] {action['action']}")
        print(f"      Framework: {action['framework']} | Timeline: {action['timeline']}")
    
    # Generate board presentation
    board_materials = reporting_suite.generate_board_presentation(regulatory_report)
    
    print(f"\nüìã 6. BOARD PRESENTATION MATERIALS")
    print(f"   Status Indicator: {board_materials['slide_1_executive_summary']['status_indicator']}")
    print(f"   Overall Score: {board_materials['slide_1_executive_summary']['overall_score']}")
    print(f"   Critical Actions: {board_materials['slide_2_risk_assessment']['critical_actions_required']}")
    print(f"   Next Deadline: {board_materials['slide_3_compliance_scorecard']['next_assessment_due']}")
    
    # Show regulatory timeline
    upcoming_events = [t for t in regulatory_report.regulatory_timeline 
                      if datetime.strptime(t["date"], "%Y-%m-%d") <= datetime.now() + timedelta(days=90)][:3]
    
    if upcoming_events:
        print(f"\nüìÖ 7. UPCOMING REGULATORY MILESTONES")
        for event in upcoming_events:
            importance_icon = "üî¥" if event["importance"] == "critical" else "üü†" if event["importance"] == "high" else "üü°"
            print(f"   {importance_icon} {event['date']}: {event['event']} ({event['framework']})")
    
    print("\n" + "="*70)
    print("üèÜ REGULATORY REPORTING DEMONSTRATION COMPLETE")
    print("üíº This showcases enterprise-ready regulatory compliance capabilities")
    print("üéØ Perfect for C-Suite Executives and Board Directors")
    print("üìû Contact Sotirios Spyrou for regulatory compliance consulting")
    print("üîó LinkedIn: https://www.linkedin.com/in/sspyrou/")
    print("üåê Enterprise Solutions: https://verityai.co")
    print("="*70)


if __name__ == "__main__":
    demonstrate_regulatory_reporting()

